<!DOCTYPE html>
<html>
<head>
<title>What's up with geom_smooth and additive models?</title>
<style>
body {
  font-family: Arial, sans-serif;
  line-height: 1.5;
  margin: 0 auto;
  max-width: 800px;
  padding: 2rem;
}

h1, h2 {
  color: #1d3557;
}

h1 {
  font-size: 3rem;
  margin-top: 0;
}

h2 {
  font-size: 2rem;
}

code {
  background-color: #f0f0f0;
  border-radius: 3px;
  font-family: monospace;
  padding: 0.1rem 0.3rem;
}

sup {
  font-size: 0.8rem;
}

figure {
  margin: 2rem 0;
}

figcaption {
  font-size: 0.8rem;
}

</style>
</head>
<body>

<h1>What's up with <code>geom_smooth</code> and additive models?</h1>

<section>
<h2>Introduction</h2>
<p>If you have ever wondered what <code>geom_smooth</code><sup><a href="#fn1" id="ref1">(Smith, 2010)</a></sup> actually does or been baffled by <i>generalised additive models</i> (GAMs) then this blog post is for you. It is not a comprehensive introduction but intended as an intuition-building stepping-stone into the more advanced literature. Michael Clark has written an elegant and more in-depth introductory blog<sup><a href="https://m-clark.github.io/generalized-additive-models/" id="ref2">(Clark, n.d.)</a></sup> while Simon Wood has written the bible <sup><a href="#fn3" id="ref3">(Wood, 2017)</a></sup>. Be warned, like the new testament the book of Wood is mostly in Greek...</p>

  
  <!-- Blog structure -->
<p>The blog first introduces and compares GAMs to its main competitor, polynomial regression. From there we proceed with some concrete examples where we manually replicate the functionality of <code>geom_smooth</code> and <span class="pkg">mgcv</span>. Lastly, we cover the central topic of penalised estimation.</p>

<h2>What are generalised additive models?</h2>

<p>Generalised additive models (GAMs) are statistical models specialized in detecting highly non-linear trends in data. In this regard, they are similar to polynomial regression which can be regarded as their main "competitor". </p>

<!-- how polynomials work -->
<p>Both polynomials and GAMs perform a <i>basis expansion</i> to detect non-linear patterns. The basis is our measured variable <em>x</em> which we expand with additional predictors to enable a non-linear fit. Formally, polynomial regression and additive models share this basic setup.</p>

<div style="text-align:center">
    <img src="images/eq1.png" alt="Equation 1" width="120" height="120">
</div>

<p><em>y</em> is our outcome variable which we assume to be drawn from a normal distribution with a mean <em>&mu;</em> and standard deviation <em>&sigma;</em>. We model <em>&mu;</em> as a sum over functions <em>f(x)</em>. This is the <i>basis expansion</i>. The parameter <em>k</em> controls the extend of the basis expansion and <em>&beta;<sub>i</sub></em> is a vector of associated coefficients. By setting <em>f(x) = x<sup>i</sup></em> we have polynomial regression. In this cases <em>k</em> controls which polynomial order we are estimating. We later see how <em>f<sub>i</sub>(x)</em> works differently for GAMs.</p>

<p>A central difference between GAMs and polynomials is that polynomials do <i>global</i> estimation. Essentially this means that, for instance, the data points at <em>x &lt; 2</em> will affect the fit at <em>x &gt; 10</em>. This global property is easy to see for linear models. Like a seesaw, we cannot change the slope at <em>x &lt; 2</em> without changing it at <em>x &gt; 10</em>. The same holds for polynomial regression. GAMs work locally and are more flexible. In the context of GAMs <em>k</em> controls how many bins we want to carve the predictor variable up into. In GAM vocabulary the points separating each bin are called <i>knots</i> and <em>k</em> is then the number of knots. GAMs fit a model to each bin - hence the local fit. These local models are then added together to obtain a single statistical model. Let us illustrate what this means.</p>

<p>All code for this example can be found at XXX. For our simulation we first need some data. In this simple example we consider an outcome <em>y</em> and predictor <em>x</em>.</p>

<pre><code>n &lt;- 200
x &lt;- runif(n)
true &lt;- 4*x + 3*x<sup>2</sup> - 3*x<sup>3</sup> - 6*x<sup>4</sup>
y &lt;- rnorm(n, true, 1.5) 
d &lt;- data.frame(x,y,true) 
</code></pre>

<figure>
    <img src="images/true+geom.pdf" alt="Figure of simulated data points and a non-linear function">
    <figcaption>200 data points (dots) simulated from a non-linear function (yellow line). The red line is the default output of <code>geom_smooth</code>. We see it recovers the relationship well.</figcaption>
    <label for="fig:true"></label>
</figure>

  Figure \ref{fig:true} shows the true non-linear relation (yellow) and a sample of 200 data points. The red line visualises the output of <code>geom_smooth</code> which recovers the true function well. Computationally <code>geom_smooth</code>  is estimated via <code>mgcv::gam</code><sup><a href="#fn1" id="ref1">1</a></sup>. The default setting of <code>geom_smooth</code> is to use a cubic regression spline as its basis function and <code>k=10</code> knots. In other words, <code>geom_smooth</code> spreads 10 cubic functions, <code>f(x) = x + x^2 + x^3</code>, across our predictor variable. In Figure \ref{fig:basis} we visualise this basis expansion. The knots are the points where one basis function equals 1 while the others equal zero. This setup makes GAMs localised as the fit of the orange colored basis functions will not depend on the data in area associated with purple basis functions.

<img src="images/basis.pdf" alt="Figure 1: A 10 knot cubic regression spline basis expansion." width="500px">

The next step is to evaluate each participant on each basis function <code>f_i(x)</code>. We can do this in R with <code>smoothCon</code> and obtain a <code>n</code> by <code>k</code> matrix (named <code>splines</code> in the code below).

<pre>
splines <- smoothCon(s(x, #predictor
                  bs="cr", #cubic regression spline
                  k = 10), #10 knots
                data=d)[[1]]$X
</pre>

The object <code>splines</code> can then be used with <code>lm</code> to estimate  <code>β</code>.<sup><a href="#fn2" id="ref2">2</a></sup> Once we know <code>β</code> we can multiply each basis function with its coefficient and sum everything to achieve our prediction.

<pre>
lmMod = lm(d$y ~ .-1, data= as.data.frame(splines)) 
#".-1" means use all variables in the data and omit adding intercept

bscoefs = coef(lmMod) #extracting coefficients

#multiplying basis functions with coefficients
bsScaled = sweep(splines, 2, bscoefs,`*`) %>% as.data.frame()  

#adding everything -> prediction
bsScaled$sum_spline  <- colSums(t(bsScaled)) 
</pre>

Figure \ref{fig:fitted} shows how the coefficients modulate the basis functions and the resulting overall model in red. In Figure \ref{fig:eval} I compare our manually fitted GAM to <code>geom_smooth</code>. As expected, their general pattern is similar, however, our model is much more "wiggly". This difference leads <i>penalised estimation</i> of GAMs.

<img src="notes_pictures/fitted_basis.png" alt="Figure 2: The 10 basis function multiplied by their coefficients obtained by fitting them to an outcome variable. The red line is the sum of the basis functions and represent the prediction of our model." width="500px">

  
  
<p>Footnotes:</p>
<div class="footnotes">
  <ol>
    <li id="fn1">
      <p>A plotting function from <code>ggplot2</code> in <span class="proglang">R</span> (<span class="cite">Smith, 2010</span>).</p>
    </li>
    <li id="fn2">
      <p><a href="https://m-clark.github.io/generalized-additive-models/">https://m-clark.github.io/generalized-additive-models/</a></p>
    </li>
    <li id="fn3">
      <p>Wood, S. N. (2017). <i>Generalized additive models: An introduction with R</i>. Chapman and Hall/CRC.</p>
    </li>
  </ol>
</div>

<div class="references">
  <ol>
          <li id="cite-R">
      <span class="cite_author">R Core Team</span>
      <span class="cite_year">(2022)</span>.
      <span class="cite_title">R: A Language and Environment for Statistical Computing</span>
      <span class="cite_organization">R Foundation for Statistical Computing</span>,
      <span class="cite_address">Vienna, Austria</span>.
      <span class="cite_url"><a href="https://www.R-project.org/">https://www.R-project.org/</a></span>.
    </li>
    <li id="cite-clark">
      <span class="cite_author">Clark, M.</span>
      <span class="cite_year">(n.d.)</span>.
      <span class="cite_title">Generalized additive models: an introduction to R</span>.
      <span class="cite_journal">Retrieved from https://m-clark.github.io/generalized-additive-models/</span>
    </li>
    <li id="cite-wood">
      <span class="cite_author">Wood, S. N.</span>
      <span class="cite_year">(2017)</span>.
      <span class="cite_title">Generalized additive models: An introduction with R</span>.
      <span class="cite_publisher">Chapman and Hall/CRC</span>.
    </li>
  </ol>
</div>
